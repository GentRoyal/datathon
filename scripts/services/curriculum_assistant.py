import os
import json
import sqlite3
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.chains import LLMChain
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

from config import Config
config = Config()

from scripts.core.database import save_assessment, save_progress, save_recommendations, get_curriculum



class AssessmentType(Enum):
    KNOWLEDGE_CHECK = "knowledge_check"
    PRACTICAL_TEST = "practical_test"

class ProficiencyLevel(Enum):
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"


class TrainingModule(BaseModel):
    """Structured training content generated by LLM"""
    topic: str = Field(description="The curriculum topic")
    explanation: str = Field(description="Clear explanation of the concept")
    teaching_methodology: str = Field(description="How to teach this effectively")
    examples: List[str] = Field(description="Practical examples")
    practice_exercises: List[str] = Field(description="Exercises for the teacher")
    key_points: List[str] = Field(description="Key takeaways")

class KnowledgeQuestion(BaseModel):
    """Knowledge check question"""
    question: str = Field(description="The assessment question")
    options: List[str] = Field(description="Multiple choice options")
    correct_answer: str = Field(description="The correct answer")
    explanation: str = Field(description="Why this is the correct answer")

class PracticalTest(BaseModel):
    """Practical teaching scenario"""
    scenario: str = Field(description="Teaching scenario description")
    task: str = Field(description="What the teacher needs to do")
    evaluation_criteria: List[str] = Field(description="How to evaluate the response")
    sample_response: str = Field(description="Example of a good response")

class AssessmentResult(BaseModel):
    """Assessment evaluation result"""
    score: float = Field(description="Score from 0-100")
    strengths: List[str] = Field(description="Areas where teacher performed well")
    weaknesses: List[str] = Field(description="Areas needing improvement")
    recommendations: List[str] = Field(description="Specific recommendations")

class LearningRecommendation(BaseModel):
    """Personalized learning recommendations"""
    topic: str = Field(description="The curriculum topic")
    priority: str = Field(description="High, Medium, or Low priority")
    recommended_actions: List[str] = Field(description="Specific actions to take")
    resources: List[str] = Field(description="Additional learning materials")


class TeacherTrainingEngine:
    """Core engine for generating training content and assessments using Groq"""
    
    def __init__(self, groq_api_key = config.GROQ_API_KEY, model: str = config.GROQ_MODEL):
        self.llm = ChatGroq(
            api_key = groq_api_key,
            model = model,
            temperature=0.7
        )
    
    def generate_training_module(self, topic: str, curriculum_context: Dict, 
                                teacher_level: str = "intermediate") -> TrainingModule:
        """Generate comprehensive training module for a topic"""
        parser = PydanticOutputParser(pydantic_object=TrainingModule)
        
        prompt = ChatPromptTemplate.from_template("""
        You are an expert teacher trainer. Create a comprehensive training module for a teacher.

        CURRICULUM CONTEXT:
        Topic: {topic}
        Description: {description}
        Prerequisites: {prerequisites}
        Learning Objectives: {learning_objectives}
        Teacher Level: {teacher_level}

        Generate a complete training module that includes:
        1. Clear explanation of the concept
        2. Effective teaching methodology
        3. Practical examples the teacher can use in class
        4. Practice exercises for the teacher to master the content
        5. Key points to remember

        {format_instructions}

        Make the training practical, engaging, and directly applicable to classroom teaching.
        """)
        
        chain = prompt | self.llm
        
        response = chain.invoke({
            "topic": topic,
            "description": curriculum_context.get("description", ""),
            "prerequisites": curriculum_context.get("prerequisites", ""),
            "learning_objectives": curriculum_context.get("learning_objectives", ""),
            "teacher_level": teacher_level,
            "format_instructions": parser.get_format_instructions()
        })
        
        return parser.parse(response.content)
    
    def generate_knowledge_check(self, topic: str, training_module: TrainingModule, 
                                 num_questions: int = 5) -> List[KnowledgeQuestion]:
        """Generate knowledge check questions"""
        parser = PydanticOutputParser(pydantic_object=KnowledgeQuestion)
        
        prompt = ChatPromptTemplate.from_template("""
        You are creating a knowledge assessment for a teacher who just completed training.

        TOPIC: {topic}
        TRAINING CONTENT: {training_content}

        Generate a challenging multiple-choice question that tests conceptual understanding of this topic.
        The question should assess whether the teacher truly understands the concept and can teach it effectively.

        {format_instructions}

        Make questions thought-provoking and relevant to actual teaching scenarios.
        """)
        
        questions = []
        for i in range(num_questions):
            chain = prompt | self.llm
            response = chain.invoke({
                "topic": topic,
                "training_content": json.dumps(training_module.dict(), indent=2),
                "format_instructions": parser.get_format_instructions()
            })
            
            questions.append(parser.parse(response.content))
        
        return questions
    
    def generate_practical_test(self, topic: str, training_module: TrainingModule) -> PracticalTest:
        """Generate practical teaching scenario"""
        parser = PydanticOutputParser(pydantic_object=PracticalTest)
        
        prompt = ChatPromptTemplate.from_template("""
        You are creating a practical teaching assessment.

        TOPIC: {topic}
        TRAINING CONTENT: {training_content}

        Create a realistic classroom scenario where the teacher must demonstrate their ability to:
        - Explain the concept clearly
        - Handle student questions
        - Use effective teaching methods
        - Engage students

        {format_instructions}

        Make the scenario challenging but realistic.
        """)
        
        chain = prompt | self.llm
        
        response = chain.invoke({
            "topic": topic,
            "training_content": json.dumps(training_module.dict(), indent=2),
            "format_instructions": parser.get_format_instructions()
        })
        
        return parser.parse(response.content)
    
    def evaluate_knowledge_assessment(self, questions: List[KnowledgeQuestion], 
                                     answers: List[str]) -> float:
        """Evaluate knowledge check answers"""
        correct = sum(1 for q, a in zip(questions, answers) if q.correct_answer.lower() == a.lower())
        return (correct / len(questions)) * 100
    
    def evaluate_practical_response(self, practical_test: PracticalTest, 
                                   teacher_response: str) -> AssessmentResult:
        """Evaluate teacher's practical response using LLM"""
        parser = PydanticOutputParser(pydantic_object=AssessmentResult)
        
        prompt = ChatPromptTemplate.from_template("""
                You are evaluating a teacher's practical teaching response.

                SCENARIO: {scenario}
                TASK: {task}
                EVALUATION CRITERIA: {criteria}
                SAMPLE GOOD RESPONSE: {sample_response}

                TEACHER'S RESPONSE:
                {teacher_response}

                Evaluate the teacher's response and provide:
                1. A score from 0-100
                2. Specific strengths in their approach
                3. Specific weaknesses or areas for improvement
                4. Actionable recommendations

                {format_instructions}

                Be constructive but honest in your evaluation.
                """)
        
        chain = prompt | self.llm
        
        response = chain.invoke({
            "scenario": practical_test.scenario,
            "task": practical_test.task,
            "criteria": json.dumps(practical_test.evaluation_criteria),
            "sample_response": practical_test.sample_response,
            "teacher_response": teacher_response,
            "format_instructions": parser.get_format_instructions()
        })
        
        return parser.parse(response.content)
    
    def generate_personalized_recommendations(self, teacher_id: str, 
                                            assessment_results: List[Tuple[str, AssessmentResult]]) -> List[LearningRecommendation]:
        """Generate personalized learning recommendations based on assessment results"""
        parser = PydanticOutputParser(pydantic_object=LearningRecommendation)
        
        # Analyze weak areas
        weak_topics = []
        for topic, result in assessment_results:
            if result.score < 70:
                weak_topics.append({
                    "topic": topic,
                    "score": result.score,
                    "weaknesses": result.weaknesses
                })
        
        recommendations = []
        for weak_topic in weak_topics:
            prompt = ChatPromptTemplate.from_template("""
            Generate personalized learning recommendations for a teacher.

            TOPIC: {topic}
            SCORE: {score}
            IDENTIFIED WEAKNESSES: {weaknesses}

            Create a focused learning recommendation with:
            1. Priority level (High/Medium/Low) based on score
            2. Specific actions the teacher should take
            3. Helpful resources and materials

            {format_instructions}

            Make recommendations practical and achievable.
            """)
            
            chain = prompt | self.llm
            
            response = chain.invoke({
                "topic": weak_topic["topic"],
                "score": weak_topic["score"],
                "weaknesses": json.dumps(weak_topic["weaknesses"]),
                "format_instructions": parser.get_format_instructions()
            })
            
            recommendations.append(parser.parse(response.content))
        
        return recommendations


# ============================================================================
# MAIN TRAINING WORKFLOW
# ============================================================================

class DynamicTeacherTraining:
    """Main class orchestrating the complete training workflow"""
    
    def __init__(self, groq_api_key: str):
        self.engine = TeacherTrainingEngine(groq_api_key)
    
    def start_training(self, teacher_id: str, class_level: str, subject: str, 
                      teacher_level: str = "intermediate"):
        """
        Complete training workflow for a teacher
        
        Args:
            teacher_id: Unique teacher identifier
            class_level: e.g., "Grade 6"
            subject: e.g., "Mathematics"
            teacher_level: beginner/intermediate/advanced
        """
        print(f"\n{'='*80}")
        print(f"DYNAMIC TEACHER CURRICULUM TRAINING")
        print(f"{'='*80}")
        print(f"Teacher ID: {teacher_id}")
        print(f"Class: {class_level} | Subject: {subject}")
        print(f"Teacher Level: {teacher_level}")
        print(f"{'='*80}\n")
        
        # 1. Retrieve curriculum
        curriculum = get_curriculum(class_level, subject)
        
        if not curriculum:
            print(f"No curriculum found for {class_level} - {subject}")
            return
        
        print(f"Found {len(curriculum)} topics in curriculum\n")
        
        # Track assessment results for final recommendations
        all_assessments = []
        
        # 2. Train on each topic
        for idx, topic_data in enumerate(curriculum, 1):
            print(f"\n{'─'*80}")
            print(f"TOPIC {idx}/{len(curriculum)}: {topic_data['topic']}")
            print(f"{'─'*80}\n")
            
            # Generate training module
            print("🎓 Generating training module...")
            training_module = self.engine.generate_training_module(
                topic_data['topic'], 
                topic_data, 
                teacher_level
            )
            
            print(f"\nTRAINING CONTENT:")
            print(f"\n{training_module.explanation}")
            print(f"\nTeaching Methodology:\n{training_module.teaching_methodology}")
            print(f"\nExamples:")

            for ex in training_module.examples[:2]:  # Show first 2
                print(f"  • {ex}")
            
            print(f"\nPractice Exercises:")
            for pe in training_module.practice_exercises[:2]:  # Show first 2
                print(f"  • {pe}")
            
            # 3. Knowledge check
            print(f"\n\nKNOWLEDGE CHECK")
            print("─" * 40)
            knowledge_questions = self.engine.generate_knowledge_check(
                topic_data['topic'], 
                training_module, 
                num_questions=3
            )
            
            # Simulate teacher answers (in real app, collect from UI)
            print("\nQuestions:")
            simulated_answers = []
            for i, q in enumerate(knowledge_questions, 1):
                print(f"\n{i}. {q.question}")
                for opt in q.options:
                    print(f"   {opt}")
                # Simulate correct answer 70% of the time
                import random
                answer = q.correct_answer if random.random() > 0.3 else random.choice(q.options)
                simulated_answers.append(answer)
                print(f"   → Teacher's answer: {answer}")
            
            knowledge_score = self.engine.evaluate_knowledge_assessment(
                knowledge_questions, 
                simulated_answers
            )
            print(f"\n✅ Knowledge Score: {knowledge_score:.1f}%")
            
            # 4. Practical test
            print(f"\n\n🎭 PRACTICAL TEACHING TEST")
            print("─" * 40)
            practical_test = self.engine.generate_practical_test(
                topic_data['topic'], 
                training_module
            )
            
            print(f"\nScenario: {practical_test.scenario}")
            print(f"\nTask: {practical_test.task}")
            
            # Simulate teacher response
            simulated_response = f"""
I would start by connecting this concept to real-world examples that students can relate to.
Then I'd break down the topic into smaller, manageable parts and use visual aids.
I'd encourage questions and use formative assessment to check understanding throughout.
Finally, I'd provide practice problems that gradually increase in difficulty.
"""
            print(f"\n[Teacher's Response]: {simulated_response.strip()}")
            
            practical_result = self.engine.evaluate_practical_response(
                practical_test, 
                simulated_response
            )
            
            print(f"\n✅ Practical Score: {practical_result.score:.1f}%")
            print(f"\n💪 Strengths:")
            for s in practical_result.strengths:
                print(f"  • {s}")
            print(f"\n⚠️  Areas for Improvement:")
            for w in practical_result.weaknesses:
                print(f"  • {w}")
            
            # 5. Save progress
            self.db.save_progress(
                teacher_id,
                topic_data['id'],
                topic_data['topic'],
                teacher_level,
                knowledge_score,
                practical_result.score
            )
            
            self.db.save_assessment(
                teacher_id,
                topic_data['topic'],
                AssessmentType.KNOWLEDGE_CHECK.value,
                AssessmentResult(
                    score=knowledge_score,
                    strengths=["Completed knowledge assessment"],
                    weaknesses=[],
                    recommendations=[]
                )
            )
            
            self.db.save_assessment(
                teacher_id,
                topic_data['topic'],
                AssessmentType.PRACTICAL_TEST.value,
                practical_result
            )
            
            all_assessments.append((topic_data['topic'], practical_result))
        
        # 6. Generate personalized recommendations
        print(f"\n\n{'='*80}")
        print("📊 PERSONALIZED LEARNING RECOMMENDATIONS")
        print(f"{'='*80}\n")
        
        recommendations = self.engine.generate_personalized_recommendations(
            teacher_id, 
            all_assessments
        )
        
        if recommendations:
            self.db.save_recommendations(teacher_id, recommendations)
            
            for rec in recommendations:
                print(f"\n🎯 Topic: {rec.topic}")
                print(f"   Priority: {rec.priority}")
                print(f"   Recommended Actions:")
                for action in rec.recommended_actions:
                    print(f"     • {action}")
                print(f"   Resources:")
                for resource in rec.resources:
                    print(f"     • {resource}")
        else:
            print("🎉 Excellent work! No additional training needed at this time.")
        
        # 7. Display dashboard summary
        self.show_teacher_dashboard(teacher_id)
    
    def show_teacher_dashboard(self, teacher_id: str):
        """Display teacher's learning dashboard"""
        print(f"\n\n{'='*80}")
        print("📈 TEACHER LEARNING DASHBOARD")
        print(f"{'='*80}\n")
        
        progress = self.db.get_teacher_progress(teacher_id)
        
        if not progress:
            print("No progress data available yet.")
            return
        
        print(f"{'Topic':<30} {'Knowledge':<12} {'Practical':<12} {'Level':<15}")
        print("─" * 80)
        
        total_knowledge = 0
        total_practical = 0
        
        for p in progress:
            print(f"{p['topic']:<30} {p['knowledge_score']:>6.1f}% {' '*5} "
                  f"{p['practical_score']:>6.1f}% {' '*5} {p['proficiency_level']:<15}")
            total_knowledge += p['knowledge_score']
            total_practical += p['practical_score']
        
        print("─" * 80)
        avg_knowledge = total_knowledge / len(progress)
        avg_practical = total_practical / len(progress)
        print(f"{'AVERAGE':<30} {avg_knowledge:>6.1f}% {' '*5} {avg_practical:>6.1f}%")
        
        print(f"\n{'='*80}\n")